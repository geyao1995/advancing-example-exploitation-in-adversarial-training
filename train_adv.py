import pprint

import torch.optim as optim

from config import *
from utils.helper_funcs import choose_model, get_trainer, set_seed, get_device, prepare_data, get_lr_scheduler
from utils.helper_funcs_wandb import *
from utils.log import Logger
from utils.tester import AdvTester


def main(config: Config):
    set_seed(config.seed.value)
    device = get_device()

    set_wandb_env(is_online=False)  # if offline, can be later synced with the `wandb sync` command.
    param_config = make_wandb_config(config)
    disable_debug_internal_log()  # need set internal log path in wandb init
    run = wandb.init(project="ExampleExploitationAT", reinit=True, dir=dir_wandb,
                     group=f'{config.data.name}',
                     job_type=f'{config.method.name}',
                     config=param_config)

    wandb.run.name = f'{wandb.run.id}'
    # wandb.run.log_code(".")  # walks the current directory and save files that end with .py.
    config_wandb = from_wandb_config(wandb.config)  # for parameter sweep
    print(f'Param config = \n'
          f'{pprint.pformat(config_wandb, indent=4)}')
    wb_metric_epoch_step, wb_metric_test_acc, wb_metric_test_rob = define_wandb_epoch_metrics()

    model = choose_model(config.data, config.model)
    model = model.to(device)
    model.train()

    train_loader, test_loader, targets, num_classes = prepare_data(config.data, train_id=True, test_id=False)
    optimizer = optim.SGD(model.parameters(), lr=config.lr_init,
                          momentum=config.momentum, weight_decay=config.weight_decay)
    lr_scheduler = get_lr_scheduler(config.total_epoch, len(train_loader), config.lr_schedule_type,
                                    optimizer, config.lr_min, config.lr_max)
    trainer = get_trainer(config.method)
    trainer = trainer(config=config, device=device,
                      model=model, train_loader=train_loader, optimizer=optimizer, num_classes=num_classes,
                      targets=targets, lr_scheduler=lr_scheduler)
    tester = AdvTester(config.param_atk_eval, device, model, test_loader)

    log = Logger(is_use_wandb=True)
    log.log_to_file(f'Param config = \n'
                    f'{pprint.pformat(config_wandb, indent=4)}')

    acc_all = []
    rob_all = []

    for i_epoch in range(1, config.total_epoch + 1):
        log.log_to_file(f'epoch: {i_epoch}')

        trainer.train_epoch(i_epoch)

        if i_epoch % 10 == 0:
            acc, rob = tester.eval()
            log.log_to_file(f'Acc: {acc:.2%}')
            log.log_to_file(f'Rob: {rob:.2%}')

            acc_all.append(acc)
            rob_all.append(rob)

            wandb.log({wb_metric_test_acc: acc, wb_metric_epoch_step: i_epoch})
            wandb.log({wb_metric_test_rob: rob, wb_metric_epoch_step: i_epoch})

        if i_epoch % 5 == 0:
            save_model_to_wandb_dir(model, i_epoch)

    save_array_to_wandb_dir(np.array(acc_all), f'acc_all')
    save_array_to_wandb_dir(np.array(rob_all), f'rob_all')
    run.finish()


if __name__ == '__main__':
    def set_lr_param(cfg: Config, lr_type: str):
        if lr_type == 'step-wise':
            cfg.lr_init = 0.1
            cfg.lr_max = 0.1
            cfg.lr_min = 0.001
            cfg.lr_schedule_type = LrSchedulerType.Step
        elif lr_type == 'cyclic-wise':
            cfg.lr_init = 0.
            cfg.lr_max = 0.2
            cfg.lr_min = 0.0
            cfg.lr_schedule_type = LrSchedulerType.Cyclic
        else:
            raise ValueError


    config = Config(
        lr_init=0.1,
        lr_max=0.1,
        lr_min=0.001,
        momentum=0.9,
        weight_decay=2e-4,
        total_epoch=100,

        param_atk_train=ConfigLinfAttack(
            epsilon=8 / 255,
            perturb_steps=1,
            step_size=10 / 255
        ),
        param_atk_eval=ConfigLinfAttack(
            epsilon=8 / 255,
            perturb_steps=50,
            step_size=2 / 255),

        lr_schedule_type=LrSchedulerType.Step,
        param_fixed_lam=6,
        param_step_size_range=ParamsStepSizeRange(
            step_size_min=2 / 255,
            step_size_max=20 / 255,
        ),
        param_lam_range=ParamsLamRange(
            lam_min=4,
            lam_max=12,
        ),
    )

    config.model = ModelName.PreActResNet_18
    config.seed = Seed.SEED_1
    config.data = DataName.CIFAR10
    method = MethodName.FastUpdated
    config.method = method
    set_lr_param(config, methods_lr_schedule[method])

    main(config)
